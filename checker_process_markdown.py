#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸€ä¸ªæ™ºèƒ½å¤„ç† Markdown æ–‡ä»¶çš„è„šæœ¬ã€‚

åŠŸèƒ½:
- è‡ªåŠ¨å¿½ç•¥ YAML Front Matterã€‚
- ä½¿ç”¨ASTï¼ˆæŠ½è±¡è¯­æ³•æ ‘ï¼‰è§£æ Markdownï¼Œç²¾å‡†è¯†åˆ«å¹¶å¿½ç•¥ä»£ç å—ã€è¡¨æ ¼ç­‰éæ®µè½å†…å®¹ã€‚
-åªæå–æ®µè½ã€åˆ—è¡¨ã€å¼•ç”¨ä¸­çš„æ–‡æœ¬ã€‚
- å°†æå–çš„å†…å®¹æŒ‰å¥å­ï¼ˆä»¥'ã€‚'ã€'ï¼'ã€'ï¼Ÿ'ç»“å°¾ï¼ŒåŒ…æ‹¬ä¸æ‹¬å·çš„ç»„åˆï¼‰åˆ†å‰²ã€‚
- å°†ç»“æœä»¥æ¯å¥ä¸€è¡Œçš„å½¢å¼è¾“å‡ºåˆ°æ–‡æœ¬æ–‡ä»¶ã€‚
"""

import argparse
import os
import re
import sys

try:
    from markdown_it import MarkdownIt
except ImportError as exc:
    print("âŒ é”™è¯¯: æ— æ³•å¯¼å…¥æ¨¡å— 'markdown_it'ã€‚", file=sys.stderr)
    print(f"   è¯¦æƒ…: {exc}", file=sys.stderr)
    print("   è¯·ä½¿ç”¨å½“å‰è§£é‡Šå™¨å®‰è£…:", file=sys.stderr)
    print(f"   {sys.executable} -m pip install markdown-it-py", file=sys.stderr)
    sys.exit(1)


def extract_text_from_markdown(file_path: str) -> list[str]:
    """
    è¯»å–Markdownæ–‡ä»¶ï¼Œæ™ºèƒ½æå–å…¶ä¸­çš„çº¯æ–‡æœ¬å†…å®¹ã€‚

    å¤„ç†æµç¨‹:
    1. è¯»å–æ–‡ä»¶å¹¶ç§»é™¤ Front Matterã€‚
    2. ä½¿ç”¨ markdown-it-py å°† Markdown è§£æä¸º token æµã€‚
     3. éå† tokenï¼Œåªæå–æ®µè½å’Œåˆ—è¡¨é¡¹ä¸­çš„æ–‡æœ¬å†…å®¹ã€‚
         è¿™æ ·å¯ä»¥è‡ªç„¶åœ°å¿½ç•¥ä»£ç å—ã€è¡¨æ ¼ã€æ ‡é¢˜ç­‰ã€‚

    Args:
        file_path (str): Markdown æ–‡ä»¶çš„è·¯å¾„ã€‚

    Returns:
        list[str]: æå–å‡ºçš„æ–‡æœ¬å—åˆ—è¡¨ï¼ˆæ¯æ®µ/é¡¹ä¸ºä¸€ä¸ªå…ƒç´ ï¼‰ã€‚
    """
    print(f"ğŸ“„ æ­£åœ¨è¯»å–å’Œè§£æè¾“å…¥æ–‡ä»¶: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ° -> {file_path}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"âŒ é”™è¯¯: è¯»å–æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ -> {e}", file=sys.stderr)
        sys.exit(1)

    # 1. ç§»é™¤ Front Matter
    parts = content.split('---', 2)
    if len(parts) == 3 and parts[0].strip() == '':
        print("  - æ£€æµ‹åˆ° Front Matterï¼Œå·²è‡ªåŠ¨å¿½ç•¥ã€‚")
        markdown_body = parts[2]
    else:
        print("  - æœªæ£€æµ‹åˆ° Front Matterï¼Œå°†å¤„ç†æ•´ä¸ªæ–‡ä»¶ã€‚")
        markdown_body = content

    # 2. ä¿æŠ¤è½¬ä¹‰å­—ç¬¦ï¼Œé¿å…è¢« markdown-it-py ç§»é™¤
    # ä½¿ç”¨ \uE000 ä½œä¸ºå ä½ç¬¦ï¼ŒåŒ¹é…åæ–œæ åè·Ÿç€ ASCII æ ‡ç‚¹ç¬¦å·çš„æƒ…å†µ
    # è¿™æ · markdown-it çœ‹åˆ°çš„æ˜¯ "\uE000" + "\X"ï¼Œå®ƒä¼šå°† \X å¤„ç†ä¸ºè½¬ä¹‰å­—ç¬¦ï¼ˆåªä¿ç•™ Xï¼‰ï¼Œ
    # æœ€ç»ˆæˆ‘ä»¬å¾—åˆ° "\uE000" + "X"ï¼Œå†å°†å…¶æ›¿æ¢å› "\X"
    markdown_body = re.sub(
        r'\\([!"#$%&\'()*+,-./:;<=>?@\[\\\]^_`{|}~])', '\uE000' + r'\\\1', markdown_body)

    # 3. ä½¿ç”¨ markdown-it-py è§£æ
    print("  - æ­£åœ¨ä½¿ç”¨ AST è§£æ Markdown ç»“æ„...")
    # å¯ç”¨è¡¨æ ¼æ”¯æŒï¼Œä»¥ä¾¿æ­£ç¡®è¯†åˆ«å’Œè¿‡æ»¤è¡¨æ ¼å†…å®¹
    md = MarkdownIt().enable('table')
    tokens = md.parse(markdown_body)

    # 4. æå–ç›®æ ‡æ–‡æœ¬
    text_blocks = []
    # åªå…³å¿ƒæ®µè½ä¸åˆ—è¡¨é¡¹å†…çš„æ–‡æœ¬
    # 'inline' token åŒ…å«äº†è¯¥å—çš„å®é™…æ–‡æœ¬å†…å®¹

    stack = []

    def _extract_inline_text(inline_token):
        # Rebuild text from inline children, preserving markdown inline styles.
        if not inline_token.children:
            return inline_token.content.replace('\uE000', '\\')
        parts = []
        link_stack = []  # ç”¨äºå¤„ç†é“¾æ¥ [text](url)

        for child in inline_token.children:
            if child.type == "image":
                if link_stack:
                    link_stack[-1]["has_image"] = True
                continue
            elif child.type == "text":
                parts.append(child.content)
            elif child.type == "html_inline":
                parts.append(child.content)
            elif child.type == "code_inline":
                # ä¿ç•™è¡Œå†…ä»£ç çš„åå¼•å·
                parts.append(f"`{child.content}`")
            elif child.type == "strong_open":
                parts.append(child.markup)
            elif child.type == "strong_close":
                parts.append(child.markup)
            elif child.type == "em_open":
                parts.append(child.markup)
            elif child.type == "em_close":
                parts.append(child.markup)
            elif child.type == "link_open":
                # è®°å½•é“¾æ¥å¼€å§‹åœ¨ parts åˆ—è¡¨ä¸­çš„ç´¢å¼•
                href = child.attrGet("href") or ""
                link_stack.append({
                    "parts_index": len(parts),
                    "href": href,
                    "has_image": False
                })
                parts.append("[")
            elif child.type == "link_close":
                if link_stack:
                    link_info = link_stack.pop()
                    if link_info.get("has_image"):
                        # å‘ç°å›¾ç‰‡ï¼Œæ’¤é”€ä» [ å¼€å§‹çš„æ‰€æœ‰æ·»åŠ 
                        parts = parts[:link_info["parts_index"]]
                    else:
                        parts.append("]")
                        parts.append(f"({link_info['href']})")
                else:
                    parts.append("]")
            elif child.type in ("softbreak", "hardbreak"):
                parts.append("\n")
        return "".join(parts).replace('\uE000', '\\')

    for token in tokens:
        if token.nesting == 1:
            stack.append(token.type)
            continue
        if token.nesting == -1:
            if stack:
                stack.pop()
            continue

        if token.type != "inline":
            continue

        inline_text = _extract_inline_text(token).strip()
        if not inline_text:
            continue

        in_paragraph = "paragraph_open" in stack
        in_list_item = "list_item_open" in stack
        in_heading = "heading_open" in stack
        # æ£€æŸ¥æ‰€æœ‰è¡¨æ ¼ç›¸å…³çš„ tokenï¼Œç¡®ä¿å®Œå…¨è¿‡æ»¤è¡¨æ ¼å†…å®¹
        table_tokens = {"table_open", "thead_open",
                        "tbody_open", "tr_open", "th_open", "td_open"}
        in_table = any(t in table_tokens for t in stack)
        in_blockquote = "blockquote_open" in stack

        # åŒ…å«å—å¼•ç”¨å†…å®¹
        if (in_paragraph or in_list_item or in_blockquote) and not in_heading and not in_table:
            text_blocks.append(inline_text)

    print(f"  - æˆåŠŸä» {len(text_blocks)} ä¸ªæ®µè½/åˆ—è¡¨é¡¹/å¼•ç”¨ä¸­æå–æ–‡æœ¬ã€‚")
    return text_blocks


def split_into_sentences(text_blocks: list[str]) -> list[str]:
    """
    å°†æ–‡æœ¬å—åˆ—è¡¨ä¸­çš„å†…å®¹åˆ†å‰²æˆå¥å­ï¼Œå¹¶è¿›è¡Œæ¸…ç†ã€‚

    ç‰¹åˆ«å¤„ç†ï¼š
    - å…¼å®¹ä¸­è‹±æ–‡åˆ†å¥è§„åˆ™ã€‚
    - å½“å¥å·ã€é—®å·ã€æ„Ÿå¹å·ä¸å³ï¼ˆåï¼‰æ‹¬å·æˆ–å¼•å·ï¼ˆå¦‚ â€ã€â€™ã€"ã€'ã€ï¼‰ç­‰ï¼‰ç»„åˆå‡ºç°æ—¶ï¼Œä»¥æœ€åä¸€ä¸ªç¬¦å·ä½œä¸ºæ–­å¥ç‚¹ã€‚
    - é’ˆå¯¹æ²¡æœ‰æ ‡ç‚¹ç»“å°¾çš„ç‹¬ç«‹æ–‡æœ¬å—ï¼ˆå¦‚æœªåŠ å¥å·çš„åˆ—è¡¨é¡¹ï¼‰ï¼Œä¹Ÿä¼šå°†å…¶ä½œä¸ºç‹¬ç«‹å¥å­ä¿ç•™ã€‚

    Args:
        text_blocks (list[str]): å¾…å¤„ç†çš„æ–‡æœ¬å—åˆ—è¡¨ã€‚

    Returns:
        list[str]: æ¸…ç†å’Œåˆ†å‰²åçš„å¥å­åˆ—è¡¨ã€‚
    """
    print("æ­£åœ¨è¿›è¡Œå¥å­åˆ†å‰²...")

    # ä¼˜åŒ–çš„ä¸­è‹±æ–‡é€šç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š
    # åŒ¹é…ä»¥ä¸‹å‡ ç§å¥å°¾æ¨¡å¼ (åŒ…å«ä¸­è‹±æ–‡æ ‡ç‚¹åŠåµŒå¥—å¼•å·/æ‹¬å·)ï¼š
    # 1. å•ç‹¬çš„å¥å°¾æ ‡ç‚¹ï¼šã€‚ï¼ï¼Ÿ.!?
    # 2. å¥å°¾æ ‡ç‚¹ + åæ‹¬å·/å¼•å·ï¼šå¦‚ ã€‚â€ | ï¼ã€ | ." | ?) | !' ç­‰
    # 3. åæ‹¬å·/å¼•å· + å¥å°¾æ ‡ç‚¹ï¼šå¦‚ â€ã€‚ | ã€ï¼ | ". | )? ç­‰
    # åŒ…å«ç¬¦å·ï¼š ï¼‰ â€ â€™ ã€ ã€ ) ] } " '
    sentence_pattern = re.compile(
        r'([^ã€‚ï¼ï¼Ÿ.!?]+?(?:[ã€‚ï¼ï¼Ÿ.!?][ï¼‰â€â€™ã€ã€"\'\)\]}]+|[ï¼‰â€â€™ã€ã€"\'\)\]}]+[ã€‚ï¼ï¼Ÿ.!?]|[ã€‚ï¼ï¼Ÿ.!?]))'
    )

    cleaned_sentences = []

    for block in text_blocks:
        # 1. å…ˆåœ¨æ•´ä¸ªæ–‡æœ¬å—ä¸­è¿‡æ»¤æ‰å¯èƒ½è·¨è¡Œçš„å…¬å¼å— $$...$$
        # ä½¿ç”¨ re.DOTALL ä½¿å¾— . å¯ä»¥åŒ¹é…æ¢è¡Œç¬¦ï¼Œä»è€Œæ­£ç¡®è¯†åˆ«è·¨è¡Œå…¬å¼
        block = re.sub(r'\$\$.+?\$\$', '', block, flags=re.DOTALL)

        # 2. å°†å¤„ç†åçš„æ–‡æœ¬å—æŒ‰æ¢è¡Œç¬¦åˆ†å‰²ï¼Œå³ä½¿åœ¨åŒä¸€ä¸ªæ®µè½ä¸­ï¼Œä¸åŒè¡Œçš„æ–‡æœ¬ä¹Ÿä¼šè¢«åˆ†å¼€å¤„ç†
        sub_lines = block.split('\n')

        for text in sub_lines:
            text = text.strip()
            if not text:
                continue

            sentences = sentence_pattern.findall(text)

            # æ£€æŸ¥æ˜¯å¦æœ‰æœªè¢«æ­£åˆ™æ•è·çš„æ®‹ç•™æ–‡æœ¬ï¼ˆå¦‚ç»“å°¾æ²¡æœ‰æ ‡ç‚¹çš„æƒ…å†µï¼‰
            # è®¡ç®—å·²æ•è·å¥å­çš„æ€»é•¿åº¦
            captured_len = sum(len(s) for s in sentences)

            if captured_len < len(text):
                remainder = text[captured_len:].strip()
                if remainder:
                    # å°†æ®‹ç•™éƒ¨åˆ†ä½œä¸ºä¸€ä¸ªæ–°å¥å­
                    sentences.append(remainder)

            # æ¸…ç†æ¯ä¸ªå¥å­ï¼Œå»é™¤å¤šä½™ç©ºç™½
            for s in sentences:
                s_cleaned = s.strip()
                if not s_cleaned:  # è·³è¿‡ç©ºå¥å­
                    continue
                # å°†å¤šä¸ªè¿ç»­ç©ºæ ¼å‹ç¼©ä¸ºå•ä¸ªç©ºæ ¼
                s_cleaned = re.sub(r'\s+', ' ', s_cleaned)
                cleaned_sentences.append(s_cleaned)

    print(f"  - æˆåŠŸåˆ†å‰²å‡º {len(cleaned_sentences)} ä¸ªå¥å­ã€‚")
    return cleaned_sentences


def write_to_txt(sentences: list[str], output_path: str, source_file_path: str):
    """
    å°†å¥å­åˆ—è¡¨å†™å…¥æŒ‡å®šçš„æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯å¥ä¸€è¡Œã€‚

        é¢å¤–è¯´æ˜:
        - ä¸ºäº†ä¾¿äºå®šä½ï¼Œæ¯å¥å‰ä¼šæ·»åŠ å¯è¿‡æ»¤çš„æ ‡ç­¾ï¼Œæ ¼å¼ä¸º: "@@S000001|filename.md@@ "ã€‚
            è¯¥æ ‡ç­¾ä½¿ç”¨å›ºå®šå‰ç¼€å’Œé›¶å¡«å……æ•°å­—ï¼Œå¹¶åŒ…å«æ¥æºæ–‡ä»¶åï¼Œæ–¹ä¾¿å®šä½ã€‚

    Args:
        sentences (list[str]): å¥å­åˆ—è¡¨ã€‚
        output_path (str): è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„ã€‚
        source_file_path (str): æ¥æº Markdown æ–‡ä»¶è·¯å¾„ã€‚
    """
    print(f"æ­£åœ¨å†™å…¥ç»“æœåˆ°: {output_path}")
    try:
        source_name = os.path.basename(source_file_path)
        with open(output_path, 'w', encoding='utf-8') as f:
            if not sentences:
                f.write("")  # å†™å…¥ç©ºæ–‡ä»¶
            else:
                tagged_lines = []
                for idx, sentence in enumerate(sentences, start=1):
                    tag = f"@@S{idx:06d}|{source_name}@@ "
                    tagged_lines.append(tag + sentence)
                f.write('\n'.join(tagged_lines) + '\n')
    except Exception as e:
        print(f"âŒ é”™è¯¯: å†™å…¥æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ -> {e}", file=sys.stderr)
        sys.exit(1)


def main():
    """
    ä¸»å‡½æ•°ï¼Œç¼–æ’æ•´ä¸ªå¤„ç†æµç¨‹ã€‚
    """
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½å¤„ç† Markdownæ–‡ä»¶ï¼šå¿½ç•¥ä»£ç /è¡¨æ ¼ï¼Œæå–æ®µè½å†…å®¹å¹¶æŒ‰å¥åˆ†å‰²ã€‚",
        epilog="ç¤ºä¾‹: python checker_process_markdown.py my_article.md output.txt"
    )
    parser.add_argument("input_file", help="è¦å¤„ç†çš„ Markdown æ–‡ä»¶ååŠè·¯å¾„ã€‚")
    parser.add_argument("output_file", help="å¯¼å‡ºçš„ txt æ–‡ä»¶ååŠè·¯å¾„ã€‚")

    args = parser.parse_args()

    # æ ¸å¿ƒå¤„ç†æµç¨‹
    extracted_text = extract_text_from_markdown(args.input_file)
    sentences_list = split_into_sentences(extracted_text)
    write_to_txt(sentences_list, args.output_file, args.input_file)

    print("\nğŸ‰å…¨éƒ¨å¤„ç†å®Œæˆï¼ç»“æœå·²æˆåŠŸä¿å­˜ã€‚")


if __name__ == "__main__":
    main()
